---
title: "Wordclouds2"
author: "aaron gowins"
date: "June 1, 2015"
output: 
  html_document:
    keep_md: true
---
## Hazir Model
Work done as part of the My-Classes Live Mentoring Program.
Check it out at: http://live.my-classes.com/ 
We will make word clouds and maybe get some other text mining done. We start with a simple example of a text file of the Appendix of Hazhir Rahmandad's paper titled "Human Growth and Body Weight Dynamics: An Integrative Systems Model". This particular appendix is especially large, since this paper presents a mathematical model. 

We have a number of libraries to load, so make sure to install.packages() if these don't look familiar. If you want to see any of the manipulations that follow call inspect(docs[1]) before and after to see the difference. Most are at least vaguely self-explanatory.

Much of this is pulled from http://onepager.togaware.com/TextMiningO.pdf, which has many other terrific posts, and I highly recommend for clarity and for being thorough but brief.

http://amunategui.github.io/pubmed-query/ also has some great info, and is a quick read.

```{r,warning=FALSE,message=FALSE,cache=FALSE}

library(SnowballC)
library(tm)
library(qdap)
library(ggplot2)
library(scales)
library(dplyr)
docs<-Corpus(DirSource("/Users/aarongowins/Desktop/txt_files"))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation) # don't usually do this
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument) #u
dtm <- DocumentTermMatrix(docs)
#tdm <- TermDocumentMatrix(docs)
freq <- colSums(as.matrix(dtm))
dtm <- removeSparseTerms(dtm, 0.5)
findFreqTerms(dtm, lowfreq=5)
findAssocs(dtm, "fat", corlimit=0.1)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
library(wordcloud)
set.seed(223)
wordcloud(names(freq),freq,min.freq=6,max.words=70, colors=brewer.pal(6,"Dark2"))
```

## PubMed Gliotoxin Search

Gliotoxin in a powerful poison produced by many strains of infectious fungi. It works by inhibiting the binding of the transcription factor NFkB, which is a key factor in human immune response.

We would like to collect all articles in PubMed that refer to gliotoxin, which, according to the obscurity of our keyword and our recollection that it was discovered in 1993, we can assume will be a fairly small collection.

RISmed is by far the most complete package for text mining in R, but there are some really cool functions in others like pubmed.mineR. These are extremely small-scale examples of what text mining can be, and real queries can take enormous space, so be ready if you want to search "cancer" to add some severe restrictions.

We will examine the titles and abstracts.


```{r,warning=TRUE,message=FALSE,cache=FALSE}
library(RISmed)
library(plyr)
p<-"gliotoxin"
res <- EUtilsSummary(p, type="esearch", db="pubmed")
date()
fetch <- EUtilsGet(res,type="efetch", db="pubmed")
count<-table(YearReceived(fetch))
count<-as.data.frame(count)
articles<-data.frame('Title'=ArticleTitle(fetch),'Abstract'=AbstractText(fetch),'Year'=YearReceived(fetch))
#head(articles,1)
abstracts<-as.character(articles$Abstract)
#head(abstracts)
#set.seed(223)
#articlesX<-split(articles,as.factor("Year"))
#head(articles$Year,1)
#dim(articles)
wordcloud(abstracts,min.freq=10,max.words=70, colors=brewer.pal(7,"Set1"))

articlesC<-aggregate(x=articles,by=list(articles$Year),FUN = function(X) paste(unique(X), collapse=", "))





  simplefunc <- function(articlesC) {
          print(articlesC$Year)
       articlesC$Abstract<-as.character(articlesC$Abstract)
       freqList<-table(articlesC$Abstract)
freqList<-sort(freqList,decreasing=TRUE)
head(freqList)
 wordcloud(articlesC$Abstract,min.freq=1,max.words=30, colors=brewer.pal(7,"Set1"))
}



articles$Abstract<-as.character(articles$Abstract)
articles$Title<-as.character(articles$Abstract)
d_ply(articlesC,.(Year),  .fun=simplefunc) 
        abstractsY<-articles[which(articles$Year==2015),]
dim(abstractsY)
       abstractsY<-as.character(abstractsY$Abstract)
#abstractsY
set.seed(223)
wordcloud(abstractsY,min.freq=1,max.words=30, colors=brewer.pal(7,"Set1"))
head(count,1)
write.table(articles,"/Users/aarongowins/Desktop/txt_files/AF.txt")
names(count)<-c("Year","Counts")
ccount <- data.frame(Year=count$Year, Counts=cumsum(count$Counts)) 
ccount$g <- "g"
names(ccount) <- c("Year","Counts","g")
head(ccount)
q <- qplot(x=Year, y=Counts, data=count, geom="bar", stat="identity")
q <- q + geom_line(aes(x=Year, y=Counts, group=g), data=ccount) +
    ggtitle(paste("PubMed articles containing \'",p,"\'", sep="")) +
    ylab("Number of articles") +
    xlab(paste("Year \n Query time: ",Sys.time(), sep="")) +
    labs(colour="") +
    theme_bw()
q + theme(legend.position=c(0.2,0.85)) +
    annotate("text", x=max(as.numeric(ccount$Year)), y=max(ccount$Counts), label=max(ccount$Counts))
```


```{r}
text<-as.character(articles)
#text<-unlist(text)
#textFREQ<-table(text)
#textFREQ<-sort(textFREQ,decreasing=TRUE)
library(stringi)
text1<-stri_count(text,regex="\\S+")
#text1<-sapply(strsplit(text, " "), length)
head(text1)
#head(text)
```
We can make a new word cloud for this search, we have added a large amount of text so we need to refresh docs. We follow the same general strategy as the first time, with parameters appropriate for a larger file. 

```
docs<-Corpus(DirSource("/Users/aarongowins/Desktop/txt_files"))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
dtm <- removeSparseTerms(dtm, .1)
t(findAssocs(dtm, "age", corlimit=0.5))
findFreqTerms(dtm, lowfreq=100)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
set.seed(123)
wordcloud(names(freq),freq,min.freq=40, colors=brewer.pal(7,"Set1"))
```